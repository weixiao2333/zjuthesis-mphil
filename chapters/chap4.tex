\chapter{引入预训练模型的联合识别}

事实证明，语言模型预训练对于学习通用语言表示很有用。 作为最新的语言模型预训练模型，BERT在许多语言理解任务中均取得了惊人的成绩，
使用双向transformer网络结构来预训练语言模型，着眼于单词左右两侧的上下文,具有更强的表达能力。可以通过附加输出层对bert进行微调来完成模型构建。
在本章，我们希望最大程度的利用bert预训练学到的语义编码能力，来提升第三章中提到的模型的准确率。

