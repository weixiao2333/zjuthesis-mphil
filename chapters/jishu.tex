\chapter{相关技术与理论基础}

\section{词向量表示}

词嵌入(word embedding)是文本词汇最流行的表示形式之一，它是特定单词的向量表示，能够捕获文档中单词的上下文，语义，句法相似性和与其他单词的关系等。
Word2Vec使用浅层神经网络学习单词嵌入，由Tomas Mikolov于2013年在Google上提出\cite{mikolov2013distributed}，
它的输入来源是一个大的单词语料库，通常输出一个具有几百个维度的向量空间，该语料库中的每个单词都在该空间中唯一分配了一个相应的向量。 
词向量位于同一向量空间中，以便在语料库中共享公共上下文的词在空间中彼此紧邻。 

  Word2Vec是具有单个隐藏层的简单神经网络，并且像所有神经网络一样具有权重，在训练过程中，其目标是调整这些权重以减少损失函数。 
  但是，Word2Vec训练时并没有处理任何任务，或者说进行的任务本身没有意义，训练完成后仅使用其隐藏层的权重，将其用作词嵌入，然后将模型的其余部分剔除。
  这个技巧在无监督的特征学习中也有用到，即训练了自编码器以在隐藏层中压缩输入向量，然后将其解压缩回输出层中，解压缩的标签任为原始向量，
  训练完成后，剥离输出层，仅使用隐藏层，因为它学习了良好的编码能力。
  如果不同的单词在上下文中相似，那么当这些单词作为输入传递进网络时，Word2Vec应该具有相似的输出，
  为了具有相似的输出，这些单词在隐藏层中计算出的单词矢量必须相似，因此Word2Vec的原理是为相似上下文中的单词学习相似的单词向量。
Word2Vec能够捕获单词之间的多个不同维度的相似度，从而可以使用词向量来表示语义和句法模式。
可以通过对这些单词的词向量进行代数运算来生成诸如“男人之于女人等于兄弟之于姐妹”的模式，
从而使“兄弟”-“男人” +“女人”的词向量与“姐妹”的词向量表示十分接近，如图\ref{fig:linear-relationships}所示。


\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{./images/linear-relationships.jpg}
  \caption{词向量之间关系\cite{mikolov2013distributed}}
  \label{fig:linear-relationships}
\end{figure}


Word2Vec是一种从原始文本中学习单词嵌入的计算效率特别高的预测模型，
它主要有两种方式，连续词袋（CBOW）模型和Skip-Gram模型。
CBOW模型：此方法将每个单词的上下文作为输入，并尝试预测与上下文相对应的单词。
如图\ref{fig:CBOW}所示，CBOW模型取用C个上下文词，输入是他们的onehot表示，乘以共享矩阵W(维度是V×N)求均值
得到的结果作为隐藏层，一个N维的长条状矩阵，隐藏层向量乘以输出权重矩阵W'得到输出向量，即为从上下文
中处在当前位置的单词。训练完成后仅使用其隐藏层的权重，将其用作词嵌入，然后将模型的其余部分剔除。
Skip-Gram模型：给定一个处在句子中单词，Skip-gram模型的训练任务是尝试预测其相邻单词,和CBOW做的处理正好相反。
可以看到，在cbow方法中，是用上下文词预测当前词，训练过程中使用梯度下降方法，不断的调整相邻上下文词的向量。
而skip-gram是用当前词来预测上下文词，训练过程中使用梯度下降法不断的调整当前词的词向量。

\begin{figure}[htbp]
  % \centering
  % \hspace{-7mm}
  \subfloat[CBOW]{
  \includegraphics[width=10cm]{./images/CBOW.jpg}
  \label{fig:CBOW}
  }
  % \hspace{5pt}
    % \hspace{+5mm}
  \subfloat[skip-gram]{
  \includegraphics[width=7cm]{./images/skip.jpg}
  \label{fig:skip}
  }
  \caption{两种词向量模型\cite{mikolov2013distributed}}
  \end{figure}

\section{循环神经网络}

\subsection{RNN}

循环神经网络（RNN）是一种特殊的神经网络，其前一步的输出将作为输入输入到当前步骤。在传统的神经网络中，
所有输入和输出都是彼此独立的，但是在需要预测句子的下一个单词的情况下，需要前一个单词的信息作为支撑，
因此需要记住前一个单词。为解决此类问题，RNN诞生了，它借助“隐藏层”解决了这个问题。
RNN的最重要的功能是“隐藏状态”，它可以记住一些有关序列的信息。

RNN有一个“内存”，可以记住有关已计算内容的所有信息。在每一层的循环中，对输入使用相同的参数，
执行相同的任务以产生输出，这大大降低了参数的复杂性。
虽然其他网络在前馈过程或反向传播过程中沿线性方向“行进”，但循环网络遵循循环关系而不是前馈传递，并通过时间使用反向传播进行学习。
循环神经网络由多个固定的激活功能单元组成，每个时间步长一个功能单元。
每个单元都有一个内部状态，称为单元的隐藏状态。此隐藏状态表示过去在给定时间步长上当前网络已掌握的知识。
此隐藏状态会在每个时间步更新，以表示网络对过去的了解做出的改变变化。

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{./images/rnn.jpg}
  \caption{RNN}
  \label{fig:rnn}
\end{figure}

上图\ref{fig:rnn}显示了将RNN展开为完整网络的过程,rnn可以被我们展开为我们想要的任意层数。例如，
如果我们输入的序列是5个单词的句子，则该网络将展开为5层神经网络，每个单词一层。对于上图具体来说，
$x_{t}$是时刻t的输入，例如，$x_{1}$ 可以是与句子的第二个单词相对应的one-hot向量。
$s_{t}$是时刻t的隐藏状态，这是网络的“记忆”，$s_{t}$根据先前的隐藏状态和当前步骤的输入来计算

\begin{equation}
  s_{t}=f(U x_{t}+W s_{t-1})
  \end{equation}

激活函数f通常是诸如tanh或ReLU之类的非线性函数,$s_{-1}$计算第一个隐藏状态所需的，通常初始化为全零。
$o_{t}$是时刻t的输出,例如，如果我们想预测句子中的下一个单词，那么它将是整个词汇表中单词的概率分布
\begin{equation}
  o_t = \mathrm softmax (V s_t)
  \end{equation}

  可以将隐藏状态$s_{t}$视为网络的记忆，$s_{t}$捕获有关先前时间步中发生的所有情况的信息。步骤t的输出$o_{t}$仅基于时间t的记忆进行计算，
  但是在实际情况中会有问题，因为$s_{t}$通常无法从太早时间之前捕获信息。与传统的深度神经网络在每一层使用不同的参数不同，
  RNN图中U，V，W在所有步骤中共享相同的参数。这是因为在每个步骤都执行相同的任务，只是输入的内容不同，这大大减少了需要学习的参数总数。
  上图在每个时间步都有输出，但是根据任务的不同，可能有些是没有必要的。例如，在预测句子的情感时，我们可能只关心最终的输出，而不关心每个单词的情感。
  同样，我们可能不需要每个时间步都输入。RNN的主要特征是其隐藏状态，该状态能够捕获有关序列的一些信息。


  
\subsection{LSTM}
  RNN的优势在于它可以将先前的信息连接到当前任务，例如使用先前的视频帧可能会有助于对当前帧的理解。
  考虑一种语言模型，该模型试图根据前一个单词预测下一个单词。
  如果试图预测“the clouds are in the sky”的最后一个词，则不需要任何进一步的上下文，很明显，下一个词将是“sky”，
  在这种情况下，如果相关信息与所需信息之间的差距很小，则RNN可以很好的学习使用过去的信息。
  但是在某些情况下，我们需要更多的上下文。考虑尝试预测文本“I grew up in France \dots I speak fluent French.”中的最后一个词,
  最临近的信息表明，下一个词可能是一种语言的名称，但是如果想缩小哪种语言的范围，需要从更远的地方来追溯“法国”的信息。相关信息与这些相关信息被需要的地方之间的差距很大是完全可能的，
不幸的是，随着差距的扩大，RNN无法处理这一问题。

长短期记忆网络（通常称为“LSTM”）是一种特殊的RNN，能够学习序列的长期依赖关系，被明确设计为能够长时间记住信息的结构，
主要解决长序列训练过程中的梯度消失和梯度爆炸问题，它是由Hochreiter＆Schmidhuber（1997）提出的，
并在随后的工作中被许多人改进和推广，它在各种问题上都表现出色，现已被广泛使用。
所有的递归神经网络都具有重复模块链的形式，在标准RNN中，此重复模块将具有非常简单的结构，例如单个tanh层。
LSTM也具有这种链状结构，但是重复模块具有与传统rnn不同的结构，它不是只有一个神经网络层，而是有四个，以非常特殊的方式进行交互。

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{./images/LSTM.jpg}
  \caption{LSTM单元模块\cite{sundermeyer2012lstm}}
  \label{fig:LSTM}
\end{figure}

LSTM结构中的关键元素是cell状态，对应图中贯穿顶部的水平线，cell像传送带一样连接起来，信息流沿整个链直线传播，只有一些较小的线性相互作用,因此信息能够非常容易地不加改变地流动。
LSTM具有删除或向单元状态添加信息的能力，这些功能由称为门的结构控制，Gates能够选择性地让信息通过，它由sigmoid神经网络层和点乘运算组成。
sigmoid输出介于0和1之间的数字，描述应允许信息流中的多少通过。值为零表示“不让任何内容通过”，而值为1表示“让所有内容通过”。
LSTM的第一步是确定要从单元状态中丢弃的信息，该决定由称为“遗忘门”的sigmoid层决定，它输入$h_{t-1}$和$x_{t}$，并对单元格$c_{t-1}$状态中的每个数字输出一个介于 0 和 1的值,
1表示“完全保留”，而0表示“完全遗忘”：
\begin{equation}
  f_{t}=σ(W_{f}\cdot[h_{t-1},x_t]+b_{f})
  \end{equation}

  下一步是确定要在cell状态下存储哪些新信息，这包括两个部分。首先，一个称为“输入门”的sigmoid层决定了我们将更新哪些值。
  接下来，tanh层创建一个新候选值的向量$\tilde{C}_{t}$，可以将其添加到状态中。之后，我们将两者结合起来以进行状态更新：
  \begin{equation}
    i_{t} =\sigma\left(W_{i} \cdot\left[h_{t-1}, x_{t}\right]+b_{i}\right) 
  \end{equation}  
    \begin{equation}
      \tilde{C}_{t} =\tanh \left(W_{C} \cdot\left[h_{t-1}, x_{t}\right]+b_{C}\right)
      \end{equation}   

然后需要更新旧单元格状态$C_{t-1}$并进入新的单元状态$C_{t}$，前面的步骤已经确定了要做什么，只需要执行即可。
将旧状态乘以 $f_{t}$，让模型忘记决定忘记的信息，然后添加$i_{t} * \tilde{C}_{t}$，这是新的候选值，
根据此决定我们更新每个状态值的大小。
\begin{equation}
C_{t}=f_{t} * C_{t-1}+i_{t} * \tilde{C}_{t}
\end{equation} 

最后需要决定要输出的内容,输出的值将基于过滤后的cell状态。
首先通过一个sigmoid层决定要输出单元状态的哪些部分，
然后通过tanh乘以sigmoid层的输出得到最终的结果，即前文提到的隐层状态$h_{t}$。
\begin{equation}
  o_{t}=\sigma\left(W_{o}\left[h_{t-1}, x_{t}\right]+b_{o}\right)
\end{equation} 
\begin{equation}
  h_{t}=o_{t} * \tanh \left(C_{t}\right)
\end{equation}


\section{注意力机制}

\section{transformer}
\section{预训练模型}
NLP的最大挑战之一是缺乏足够的训练数据，从全局视角来看有大量的文本数据可用，但是如果我们要创建特定任务的数据集，则需要将总的数据堆划分为很多不同的小块。
而当我们这样做时，我们最终仅得到数千或数万个人为标记的培训示例。不幸的是，为了表现良好，基于深度学习的NLP模型需要大量的数据，在数百万或数亿的带注释的训练样本上进行训练时，
模型效果相比少数据量会有质的提升。为了帮助弥合数据鸿沟，研究人员开发了多种技术，可在网络上使用大量无注释的文本来训练通用语言表示模型（这称为预训练），
然后，可以在较小的特定任务的数据集上微调这些通用的预训练模型，例如，在处理诸如问答系统和情感分析之类的问题时。与从头开始对较小的特定于任务的数据集进行训练相比，
此方法可显着提高准确性。BERT是这些技术中用于NLP预训练的最新功能，它在深度学习社区引起了轰动，因为它在各种NLP任务中都提供了最优异的成果。

语言建模的真正意义是什么？语言模型试图解决哪些问题？基本上，他们的任务是根据上下文“填补空白”。例如，给定
“The woman went to the store and bought a（ ）of shoes”，语言模型可能会说“cart”一词将在20％的情况合适，而“pair”一词将在80％的情况合适。
在BERT之前的世界中，语言模型会在训练期间从左到右或从左到右和从右到左的组合查看此文本序列。这种单向方法很适合生成句子，我们可以预测下一个单词，
将其附加到序列中，然后预测下一个单词的下一个单词，直到获得完整的句子。
现在进入BERT时期，这是一种经过双向训练的语言模型（这也是其关键的技术创新），这意味着与单向语言模型相比，我们现在可以对语言上下文和流程有更深刻的了解。
BERT并没有预测序列中的下一个单词，而是使用了一种称为Masked LM（MLM）的新颖技术：它随机屏蔽句子中的单词，然后尝试预测它们。
掩蔽意味着该模型从两个方向看，并且它使用句子的整个上下文（左右环境）来预测被掩盖的单词。与以前的语言模型不同，它同时考虑了上一个和下一个词 。
基于LSTM的从左到右和从右到左组合的现有模型缺少此能力。（不过，说BERT是无方向性的可能更准确。）

但是，为什么这种非定向方法如此强大？预训练的语言表示可以是上下文无光或基于上下文的。上下文相关的模型又可以分为单向和双向的。
word2vec之类的上下文无关模型会为词汇表中的每个单词生成单个单词嵌入表示（数字矢量）。
例如，单词“bank”在“bank account”和“bank of the river”中将具有相同的上下文无关表示。
另一方面，基于上下文的模型会基于句子中的其他单词生成每个单词的表示形式。例如，在句子““I accessed the bank account”，
单向上下文模型将基于“I accessed the”而不是“account”来表示“bank” 。但是，BERT从前部和后部上下文信息（“I accessed the … account”）表示“bank”，
它从深度神经网络的最底层开始，使其深度双向化。

BERT依赖于Transformer（一种学习文本中单词之间的上下文关系的注意力机制），一个基本的Transformer由一个读取文本输入的编码器和一个对任务进行预测的解码器组成。
由于BERT的目标是生成语言表示模型，因此只需要编码器部分即可。BERT编码器的输入是一系列token，这些token首先被转换为矢量，然后在神经网络中进行处理。
但是在开始处理之前，BERT需要对输入进行处理并用一些额外的元数据修饰：

1.令牌嵌入：在第一个句子的开头将[CLS]令牌添加到输入的单词令牌中，并在每个句子的末尾插入[SEP]令牌。

2.段嵌入：将指示句子A或句子B的标记添加到每个标记，这使编码器能够区分句子。

3.位置嵌入：将位置嵌入添加到每个标记，以指示其在句子中的位置。
\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{./images/inputBert.jpg}
  \caption{bert的输入处理}
  \label{fig:inputBert}
\end{figure}

本质上，Transformer会堆叠一个将序列映射到序列的层，因此输出也是在相同索引处的输入和输出令牌之间具有一一对应关系的向量序列。
而且正如我们之前所了解的，BERT不会尝试预测句子中的下一个单词。培训使用以下两种策略：Masked LM (MLM)，这里的想法是简单的，
随机掩盖输入中15％的单词（用[MASK]令牌代替），通过基于BERT注意的编码器运行整个序列，然后根据序列中其他未屏蔽单词提供的上下文预测被屏蔽的词，
但是，这种掩盖方法存在一个问题，模型仅尝试预测输入中存在的词，而我们希望模型尝试预测正确的token，而不管该token是否存在输入中。
要解决此问题，bert在训练时，实际上有80％的tokenb被替换为令牌[MASK]，10％的令牌被替换为随机令牌，10％的令牌保持不变。Next Sentence Prediction，
为了理解两个句子之间的关系，BERT训练过程还使用下一个句子预测，具有这种理解的预训练模型与诸如回答问题之类的任务有关。
在训练过程中，该模型将句子对作为输入，并学习预测第二个句子是否是原始文本中的下一个句子。第二句出现在第一句之后的概率有50％，剩下50％出现的是语料库中随机的句子。
为了预测第二句话是否与第一句话相连，基本上整个输入序列都会经过基于Transformer的模型，使用简单的分类层将[CLS]令牌的输出转换为2×1形状的矢量，
并使用softmax分配IsNext-Label。